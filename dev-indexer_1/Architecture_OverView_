# ZenGlow AI Workspace - Architecture Overview

_Last Updated: September 8, 2025_

## Project Structure

```
ZenGlowAIWorkspace/
‚îú‚îÄ‚îÄ üéØ Core Application
‚îÇ   ‚îú‚îÄ‚îÄ app/                     # Main FastAPI application
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core/               # Core services (config, metrics, cache)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag/                # RAG pipeline & retrieval
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ leonardo/           # Voice-enabled Leonardo assistant
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ audio/              # TTS/STT integration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ health/             # Health monitoring
‚îÇ   ‚îÇ
‚îú‚îÄ‚îÄ üß† Fine-Tuning Infrastructure
‚îÇ   ‚îú‚îÄ‚îÄ fine_tuning/            # Specialized model training
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ datasets/           # Training data (Socratic, drill-down, interruption)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/             # Trained specialist models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training/           # Training scripts & workflows
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validation/         # Model validation & testing
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tooling/            # LLM-as-Judge, coordination tools
‚îÇ   ‚îÇ
‚îú‚îÄ‚îÄ üóÑÔ∏è Data & Knowledge
‚îÇ   ‚îú‚îÄ‚îÄ data/                   # Raw datasets & samples
‚îÇ   ‚îú‚îÄ‚îÄ sql/                    # Database schemas
‚îÇ   ‚îú‚îÄ‚îÄ artifact/               # Knowledge graph artifacts
‚îÇ   ‚îî‚îÄ‚îÄ memory_snapshot.json    # System state snapshots
‚îÇ   ‚îÇ
‚îú‚îÄ‚îÄ üèóÔ∏è Infrastructure
‚îÇ   ‚îú‚îÄ‚îÄ infrastructure/         # Deployment configs
‚îÇ   ‚îú‚îÄ‚îÄ scripts/                # Automation & utilities
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml      # Container orchestration
‚îÇ   ‚îî‚îÄ‚îÄ Makefile               # Build automation
‚îÇ   ‚îÇ
‚îú‚îÄ‚îÄ üìö Documentation
‚îÇ   ‚îú‚îÄ‚îÄ docs/                   # Technical documentation
‚îÇ   ‚îú‚îÄ‚îÄ README.md               # Project overview
‚îÇ   ‚îú‚îÄ‚îÄ DEVOPS.md              # Operations guide
‚îÇ   ‚îî‚îÄ‚îÄ DEVOPS_TODO_HISTORY.md # Operational history
‚îÇ   ‚îÇ
‚îî‚îÄ‚îÄ üß™ Development
    ‚îú‚îÄ‚îÄ tests/                  # Test suites
    ‚îú‚îÄ‚îÄ frontend/               # Web UI components
    ‚îî‚îÄ‚îÄ archive/                # Archived/deprecated code
```

## Architecture: Specialized Models + RAG Integration

### üéØ Core Concept

**Specialized Models** = Expert interaction patterns
**RAG System** = Contextual knowledge provider
**Integration** = Specialists leverage RAG for domain-relevant context

### üîÑ Data Flow Architecture

```mermaid
graph TB
    User[User Input] --> Router[Model Router]
    Router --> Specialist{Specialist Selection}

    Specialist -->|Socratic| SocMod[Socratic Model]
    Specialist -->|Drill-Down| DrillMod[Drill-Down Model]
    Specialist -->|Interruption| IntMod[Interruption Model]
    Specialist -->|General| GenMod[General Model]

    SocMod --> RAG[RAG Context Retrieval]
    DrillMod --> RAG
    IntMod --> RAG
    GenMod --> RAG

    RAG --> Vector[Vector Store<br/>pgvector]
    RAG --> Context[Contextual Response]

    Context --> SocMod
    Context --> DrillMod
    Context --> IntMod
    Context --> GenMod

    SocMod --> Response[Specialized Response]
    DrillMod --> Response
    IntMod --> Response
    GenMod --> Response
```

### üß† Specialized Model Types

| Specialist               | Purpose                 | Training Data                | RAG Integration                              |
| ------------------------ | ----------------------- | ---------------------------- | -------------------------------------------- |
| **Socratic Tutor**       | Question-based learning | 846 examples                 | Retrieves curriculum context for questioning |
| **Drill-Down Expert**    | Intent probing          | 648 examples                 | Gets background knowledge for deeper inquiry |
| **Interruption Handler** | Graceful interruptions  | 71 examples                  | Retrieves interrupted topic context          |
| **Base Foundation**      | Core methodology        | 500 pure + 1,842 personality | General knowledge retrieval                  |

### üèóÔ∏è System Architecture Layers

1. **API Layer** (FastAPI)
   - Model routing & selection
   - Request/response handling
   - Health monitoring

2. **Specialist Layer** (Fine-tuned Models)
   - Domain-specific interaction patterns
   - Specialized response generation
   - Context-aware behavior

3. **RAG Layer** (Retrieval & Context)
   - Vector similarity search (pgvector)
   - Contextual knowledge retrieval
   - Domain-specific content filtering

4. **Knowledge Layer** (Storage)
   - Vector embeddings store
   - Domain knowledge bases
   - Conversation memory

5. **Infrastructure Layer** (DevOps)
   - Container orchestration (Docker; optional sidecars via compose profiles)
   - Model serving (Ollama for Leonardo at LEONARDO_URL; model via LEONARDO_MODEL)
   - Caching (Redis)
   - Monitoring & metrics

### üéì Example: Socratic Math Tutoring

```
1. Student: "I don't understand quadratic equations"
2. Router: Selects Socratic Specialist
3. RAG: Retrieves quadratic equation concepts, common misconceptions
4. Socratic Model + Context: "What do you think happens when we have x¬≤?
   Have you worked with simpler equations like x + 3 = 7 before?"
5. Response: Contextually-informed Socratic questioning sequence
```

### üîß Technical Integration

**Interruption Handling Example:**

```
1. TTS playing explanation ‚Üí User speaks ‚Üí [USER_INTERRUPTION] token
2. Application Controller: Pause TTS, capture user input
3. Interruption Specialist: Process interruption gracefully
4. RAG: Retrieve context about interrupted topic
5. Response: "Great question! Let me address that..."
```

### üìä Current Capabilities

**Operational:**

- ‚úÖ Docker Compose stack (backend; optional sidecars profiles: `leonardo` (Ollama/Mistral), `whisper` (Whisper.cpp))
- ‚úÖ Leonardo voice integration (TTS/Whisper; can be disabled with SKIP_AUDIO_IMPORTS=1)
- ‚úÖ RAG pipeline with pgvector
- ‚úÖ Metrics & health monitoring
 - ‚úÖ Redis cache: local container, ZFS-backed persistence via `REDIS_DATA_DIR`
 - ‚úÖ DB schemas mounted on init (core + PII); Supabase sync scripts for core/PII
   - ‚úÖ Ingestion spool automation (docs/INGESTION_SPOOL.md): local dev structure provided under `data/spool/`; watcher + process orchestrator scripts in `scripts/`

**Training Infrastructure:**

- ‚úÖ Organized fine-tuning workspace
- ‚úÖ 4 specialized training datasets ready
- ‚úÖ LLM-as-Judge validation framework (Mistral7b)
- ‚úÖ Base + specialization training strategy

**Memory Management:**

- ‚úÖ Versioned knowledge graph snapshots
- ‚úÖ MCP Memory ‚Üí RAG integration bridge
- ‚úÖ Automated project indexing

### üöÄ Next Phase: Training Automation

**Priority Queue:**

1. **Model Training Pipeline**: Automate base + specialization training
2. **Deployment Integration**: Deploy specialists alongside RAG
3. **Model Router**: Implement specialist selection logic
4. **Performance Monitoring**: Track specialist effectiveness
5. **Continuous Learning**: Feedback loops for model improvement

---

_This architecture enables domain experts (Socratic tutors, drill-down questioners) to leverage contextual knowledge (RAG) for specialized, intelligent interactions._

Notes:

- To run without GPU models during training: set `LLM_DISABLE=ollama,llama,llama.cpp`, `SKIP_AUDIO_IMPORTS=1`, and leave sidecar profiles stopped.
- To enable Leonardo: start compose profile `leonardo` and set `LEONARDO_URL` / `LEONARDO_MODEL` as needed.
- To enable Whisper server: start compose profile `whisper` and set `WHISPER_SERVER_URL`.
 - Upcoming: Docker Swarm light deploy (<=4 CPU, ~20 GiB). Minimal services (app, db, db_pii, redis, whisper CPU, embed sidecar bsg-small, voice TTS). Phi-3-mini CUDA to be defined but not deployed until GPU node available.
